{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0skI-nk54UH"
      },
      "source": [
        "Clone the github repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVLI5KUx2HL5",
        "outputId": "eefb4c8f-374d-439b-8140-ac65be9684d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'IR_Repo'...\n",
            "remote: Enumerating objects: 63783, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 63783 (delta 49), reused 71 (delta 37), pack-reused 63687\u001b[K\n",
            "Receiving objects: 100% (63783/63783), 2.34 GiB | 35.43 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n",
            "Updating files: 100% (63288/63288), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone --branch scratch https://github.com/cssaivishnu/IR_Repo.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9e4eRhtB52N"
      },
      "source": [
        "Import the essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERQFmJW3IqSN",
        "outputId": "c755d741-a624-427f-f79a-a08191115700"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gkxVB3cmG3lM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "\n",
        "tf.random.set_seed(20)\n",
        "random.seed = 20\n",
        "np.random.seed = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqF-6BAL4387"
      },
      "source": [
        "Make the main repo as the current active repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqKEWeFM4387",
        "outputId": "a3e574d7-064d-4543-e722-e1e463008e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/IR_Repo\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/IR_Repo')\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3vWKcjbY9R9"
      },
      "source": [
        "From the complete dataset of approximately 63285 images from 35 categories, we consider only the product categories with atleast 150 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8txyzYs7Yyj4"
      },
      "outputs": [],
      "source": [
        "def func(name):\n",
        "    for i in range(len(name)):\n",
        "        if name[i] == '&' or name[i] == '-':\n",
        "            name = name[:i] + '_' + name[i+1:]\n",
        "    return name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOoxFhBQN1w2",
        "outputId": "21733e54-7722-4b7b-98bc-98af49667ae7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Number of categories: 35\n",
            "Number of categories with atleast 150 images: 22\n",
            "Number of categories with 600 images: 18\n",
            "Total Number of Images: 63285\n"
          ]
        }
      ],
      "source": [
        "dir_path = 'atlas_dataset_full'\n",
        "dir_count = 0\n",
        "active_dir_count = 0\n",
        "complete_dir_count = 0\n",
        "total_images = 0\n",
        "images_list = []\n",
        "\n",
        "os.mkdir('dataset')\n",
        "\n",
        "for name in sorted(os.listdir(dir_path)):\n",
        "    if os.path.isdir(os.path.join(dir_path, name)):\n",
        "        path = os.path.join(dir_path, name)\n",
        "        path = os.path.join(path, 'images')\n",
        "        num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
        "        if(num_files > 150):\n",
        "            active_dir_count = active_dir_count + 1\n",
        "            name = func(name)\n",
        "            new_path = os.path.join('dataset', name)\n",
        "            os.mkdir(new_path)\n",
        "            i = 0\n",
        "            for f in os.listdir(path):\n",
        "                if i == 600:\n",
        "                    complete_dir_count = complete_dir_count + 1\n",
        "                    break\n",
        "                if os.path.isfile(os.path.join(path, f)):\n",
        "                    i = i + 1\n",
        "                    src = os.path.join(path, f)\n",
        "                    fname = '{:03d}'.format(i) + '.' + f.split('.')[-1]\n",
        "                    dst = os.path.join(new_path, fname)\n",
        "                    shutil.copy2(src, dst)\n",
        "            images_list.append(i)\n",
        "        total_images = total_images + num_files\n",
        "        dir_count += 1\n",
        "\n",
        "print(\"Total Number of categories:\", dir_count)\n",
        "print(\"Number of categories with atleast 150 images:\", active_dir_count)\n",
        "print(\"Number of categories with 600 images:\", complete_dir_count)\n",
        "print(\"Total Number of Images:\", total_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9-2Y9LG_df2"
      },
      "source": [
        "We find that 22 out of these 35 categories only have atleast 150 images\n",
        "\n",
        "We want to have 600 images from each of the 22 product categories, out of which 4 of them have less than 600 images.\n",
        "\n",
        "Now, we will perform image augmentation to increase the number of images in those 4 product categories with less than 600 images to 600 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a2b_YKONJFek"
      },
      "outputs": [],
      "source": [
        "# Here, the image is flipped horizontally to create a new image\n",
        "\n",
        "def horizontalflipping_augmentation(path, new_path):\n",
        "    # Define the horizontal flipping transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "    ])\n",
        "\n",
        "    # Load the image\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # Apply the horizontal flipping transformation\n",
        "    img_flipped = transform(img)\n",
        "\n",
        "    # Display the original and flipped images\n",
        "    # img.show()\n",
        "    # img_flipped.show()\n",
        "\n",
        "    # Save the horizontally flipped image\n",
        "    img.save(new_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bISEXc2PFUaY"
      },
      "outputs": [],
      "source": [
        "# Here, the image is modified by varying color glittering entities like contrast, brightness etc.\n",
        "\n",
        "def colorgittering_augmentation(path,new_path):\n",
        "    # Load the image\n",
        "    img = cv2.imread(path)\n",
        "\n",
        "    # Define the range of color jittering values\n",
        "    brightness = 0.1\n",
        "    contrast = 0.1\n",
        "    saturation = 0.1\n",
        "    hue = 0.1\n",
        "\n",
        "    # Convert the image from BGR to HSV color space\n",
        "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Apply color jittering to the image\n",
        "    img_hsv[:, :, 2] = np.clip(img_hsv[:, :, 2] * (1 + brightness), 0, 255)\n",
        "    img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1] * (1 + contrast), 0, 255)\n",
        "    img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1] * (1 + saturation), 0, 255)\n",
        "    img_hsv[:, :, 0] = np.clip(img_hsv[:, :, 0] * (1 + hue), 0, 255)\n",
        "\n",
        "    # Convert the image back to BGR color space\n",
        "    img_jittered = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    # Display the original and jittered images\n",
        "    # cv2_imshow(img)\n",
        "    # cv2_imshow(img_jittered)\n",
        "    # cv2.waitKey(0)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    # Save the color gritted image\n",
        "    cv2.imwrite(new_path, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XcdGedzTOWDM"
      },
      "outputs": [],
      "source": [
        "# Here, the image is randomly scaled to a feasible size\n",
        "\n",
        "def randomscaling_augmentation(path,new_path):\n",
        "    # Load the image\n",
        "    img = Image.open(path)\n",
        "    width, height = img.size\n",
        "    size = int(min(width,height)*0.9)\n",
        "\n",
        "    # Define the random scaling transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=size, scale=(0.8, 1.0)),\n",
        "    ])\n",
        "\n",
        "    # Apply the random scaling transformation\n",
        "    img_scaled = transform(img)\n",
        "\n",
        "    # Display the original and scaled images\n",
        "    # img.show()\n",
        "    # img_scaled.show()\n",
        "\n",
        "    # Save the randomly scaled image\n",
        "    img.save(new_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8EZYZlEw-zm3"
      },
      "outputs": [],
      "source": [
        "dir_path = 'dataset'\n",
        "images_list = []\n",
        "name_list = sorted(os.listdir(dir_path))\n",
        "\n",
        "for name in name_list:\n",
        "    if os.path.isdir(os.path.join(dir_path, name)):\n",
        "        path = os.path.join(dir_path, name)\n",
        "        num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
        "        images_list.append(num_files)\n",
        "        if num_files != 600:\n",
        "            # print(num_files)\n",
        "            # print(name)\n",
        "            for f in os.listdir(path):\n",
        "                if os.path.isfile(os.path.join(path, f)):\n",
        "                    img_name = int(f.split('.')[0])\n",
        "                    for i in range(1,4):\n",
        "                        new_img_name = img_name + num_files*i\n",
        "                        if new_img_name > 600:\n",
        "                            break\n",
        "                        new_img_name = '{:03d}'.format(new_img_name) + '.' + f.split('.')[-1]\n",
        "                        img_path = os.path.join(path,f)\n",
        "                        new_img_path = os.path.join(path,new_img_name)\n",
        "                        if i == 1:\n",
        "                            horizontalflipping_augmentation(img_path,new_img_path)\n",
        "                        if i == 2:\n",
        "                            colorgittering_augmentation(img_path,new_img_path)\n",
        "                        if i == 3:\n",
        "                            randomscaling_augmentation(img_path,new_img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dBb7kdLvGf1"
      },
      "source": [
        "Create the label encodings mapping the product categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eDllHTjgUGMJ"
      },
      "outputs": [],
      "source": [
        "dir_path = 'dataset'\n",
        "i = 0\n",
        "name_to_label = dict()\n",
        "label_to_name = dict()\n",
        "\n",
        "for name in name_list:\n",
        "    if os.path.isdir(os.path.join(dir_path, name)):\n",
        "        name_to_label[name] = i\n",
        "        label_to_name[i] = name\n",
        "        i = i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA_aLZBZwCRs"
      },
      "source": [
        "Load 500 out of 600 images from each product category and modify them to (56, 56) shape for training and evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MdWasMiGBIoY"
      },
      "outputs": [],
      "source": [
        "dir_path = 'dataset'\n",
        "dataset = []\n",
        "image_shape = (56, 56)\n",
        "\n",
        "for name in name_list:\n",
        "    if os.path.isdir(os.path.join(dir_path, name)):\n",
        "        path = os.path.join(dir_path, name)\n",
        "        # print(name_to_label[name])\n",
        "        for f in os.listdir(path):\n",
        "            if os.path.isfile(os.path.join(path, f)):\n",
        "                num = int(f.split('.')[0])\n",
        "                if num > 500:\n",
        "                    continue\n",
        "                img_path = os.path.join(path, f)\n",
        "                img = Image.open(img_path)\n",
        "                img = img.resize(image_shape, Image.ANTIALIAS)\n",
        "                pixels = img.load()\n",
        "                lst = []\n",
        "                for i in range(img.size[0]):\n",
        "                    lst1 = []\n",
        "                    for j in range(img.size[1]):\n",
        "                        lst1.append(list(pixels[i, j]))\n",
        "                    lst.append(lst1)\n",
        "                lst = np.array(lst)\n",
        "                tupl = (lst,name_to_label[name])\n",
        "                dataset.append(tupl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHqNOE3pIMTD"
      },
      "source": [
        "Shuffle the dataset for randomness and split the dataset into features(X) and labels(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ebBGsaQz4YBr"
      },
      "outputs": [],
      "source": [
        "random.shuffle(dataset)\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for tupl in dataset:\n",
        "    x, yy = tupl\n",
        "    X.append(x)\n",
        "    y.append(yy)\n",
        "X = np.array(X)\n",
        "y = np.array(y).reshape(11000,1)\n",
        "\n",
        "# Just clear the space\n",
        "# dataset = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ank6_wq74388"
      },
      "source": [
        "Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C51FYTJiNkgt"
      },
      "outputs": [],
      "source": [
        "input_shape = (56, 56, 3)\n",
        "num_categories = len(name_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRS1mudkPFuk"
      },
      "source": [
        "Split the dataset into train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyQrQMexPKFy",
        "outputId": "0b11e0dc-be67-452d-9f73-e3bad40fed42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (8800, 56, 56, 3)\n",
            "X_test: (2200, 56, 56, 3)\n",
            "y_train: (8800, 1)\n",
            "y_test: (2200, 1)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print('X_train:', X_train.shape)\n",
        "print('X_test:', X_test.shape)\n",
        "print('y_train:', y_train.shape)\n",
        "print('y_test:', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJhF1icQPgqN"
      },
      "source": [
        "Image Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UMt-JY3iLtq2"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip('horizontal'),\n",
        "        layers.RandomRotation(0.02),\n",
        "        layers.RandomWidth(0.2),\n",
        "        layers.RandomHeight(0.2)\n",
        "    ]\n",
        ")\n",
        "\n",
        "data_augmentation.layers[0].adapt(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsxbj2dvRFXV"
      },
      "source": [
        "Define the Supervised Contrastive Loss Function that will be used in training the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rR3Y2t3wS4pQ"
      },
      "outputs": [],
      "source": [
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # normalize the feature vectors\n",
        "        feature_vectors_normailzed = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute Logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normailzed, tf.transpose(feature_vectors_normailzed)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ES_vSeYNZY"
      },
      "source": [
        "Create an encoder to encode the images pixel data using the RNN - ResNet50V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vfqUfbAOXeOK"
      },
      "outputs": [],
      "source": [
        "def create_encoder():\n",
        "    resnet = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=None, input_shape=input_shape, pooling='avg'\n",
        "    )\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = resnet(augmented)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='encoder')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSpCGwfQdAEa"
      },
      "source": [
        "Adding a Projection head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QkKIGzMEaehd"
      },
      "outputs": [],
      "source": [
        "def add_projection_head(encoder, projection_units):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    outputs = layers.Dense(projection_units, activation='relu')(features)\n",
        "    model = keras.Model(\n",
        "        inputs=inputs, outputs=outputs, name='encoder_with_projection_head'\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrr30sSHfMig"
      },
      "source": [
        "Train the encoder with Supervised Contrastive Loss defined above for better encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEwNpfm5d2nd",
        "outputId": "b0b3e985-7ce5-435c-a2e8-e813be2e7f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 56, 56, 3)]       0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, None, None, 3)     7         \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,564,807\n",
            "Trainable params: 23,519,360\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Model: \"encoder_with_projection_head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 56, 56, 3)]       0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 2048)              23564807  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               524544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,089,351\n",
            "Trainable params: 24,043,904\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - 95s 485ms/step - loss: 4.1223\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - 13s 151ms/step - loss: 3.7233\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - 13s 151ms/step - loss: 3.4964\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - 12s 140ms/step - loss: 3.3437\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 3.2154\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - 11s 126ms/step - loss: 3.1551\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 3.0959\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - 10s 117ms/step - loss: 3.0135\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - 11s 120ms/step - loss: 2.9619\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - 11s 121ms/step - loss: 2.9136\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - 10s 115ms/step - loss: 2.9026\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - 10s 113ms/step - loss: 2.8395\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - 10s 117ms/step - loss: 2.8155\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - 10s 113ms/step - loss: 2.7787\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - 10s 110ms/step - loss: 2.7596\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.7343\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - 10s 111ms/step - loss: 2.7096\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - 9s 108ms/step - loss: 2.6902\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - 10s 110ms/step - loss: 2.6601\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.6413\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.6178\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - 10s 108ms/step - loss: 2.6020\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - 10s 114ms/step - loss: 2.5906\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.5797\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - 9s 108ms/step - loss: 2.5714\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - 10s 115ms/step - loss: 2.5702\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - 10s 109ms/step - loss: 2.5285\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.5124\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - 10s 110ms/step - loss: 2.4767\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.4847\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.4915\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.4507\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - 10s 115ms/step - loss: 2.4577\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 2.4494\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - 10s 113ms/step - loss: 2.4526\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - 9s 108ms/step - loss: 2.3977\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.4081\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.4076\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.3791\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.3719\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - 9s 104ms/step - loss: 2.3814\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.3475\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - 10s 109ms/step - loss: 2.3260\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.3217\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - 10s 108ms/step - loss: 2.3096\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.3105\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.3137\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.3034\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.2806\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.2601\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "projection_units = 256\n",
        "epochs = 50\n",
        "temperature = 0.05\n",
        "\n",
        "encoder = create_encoder()\n",
        "encoder.summary()\n",
        "encoder_with_projection_head = add_projection_head(encoder, projection_units)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature)\n",
        ")\n",
        "encoder_with_projection_head.summary()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    history = encoder_with_projection_head.fit(\n",
        "        x=X_train, y=y_train, batch_size=batch_size, epochs=epochs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEU6SyBaBwj3"
      },
      "source": [
        "Creating and training the classifier using the pretrained encoder, and saving the model into a .h5 file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FERzd63ofL12"
      },
      "outputs": [],
      "source": [
        "def create_classifier(encoder, dropout_rate, hidden_units, learning_rate, trainable=True):\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(hidden_units, activation='relu')(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    outputs = layers.Dense(num_categories, activation='softmax')(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name = 'classifier')\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Mq2HYPN5uo",
        "outputId": "a478eaa5-0f1d-48b9-843e-63fd92e22720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 56, 56, 3)]       0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 2048)              23564807  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               1049088   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 22)                11286     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,625,181\n",
            "Trainable params: 1,060,374\n",
            "Non-trainable params: 23,564,807\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "71/71 [==============================] - 11s 94ms/step - loss: 1.0060 - sparse_categorical_accuracy: 0.7061 - val_loss: 0.4693 - val_sparse_categorical_accuracy: 0.8403\n",
            "Epoch 2/100\n",
            "71/71 [==============================] - 4s 51ms/step - loss: 0.6909 - sparse_categorical_accuracy: 0.7661 - val_loss: 0.4891 - val_sparse_categorical_accuracy: 0.8233\n",
            "Epoch 3/100\n",
            "71/71 [==============================] - 4s 50ms/step - loss: 0.6710 - sparse_categorical_accuracy: 0.7651 - val_loss: 0.4551 - val_sparse_categorical_accuracy: 0.8313\n",
            "Epoch 4/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.6342 - sparse_categorical_accuracy: 0.7756 - val_loss: 0.4228 - val_sparse_categorical_accuracy: 0.8466\n",
            "Epoch 5/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.6218 - sparse_categorical_accuracy: 0.7773 - val_loss: 0.4188 - val_sparse_categorical_accuracy: 0.8528\n",
            "Epoch 6/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5874 - sparse_categorical_accuracy: 0.7989 - val_loss: 0.4099 - val_sparse_categorical_accuracy: 0.8511\n",
            "Epoch 7/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5741 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4136 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 8/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5686 - sparse_categorical_accuracy: 0.7936 - val_loss: 0.4061 - val_sparse_categorical_accuracy: 0.8580\n",
            "Epoch 9/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5584 - sparse_categorical_accuracy: 0.7969 - val_loss: 0.4171 - val_sparse_categorical_accuracy: 0.8398\n",
            "Epoch 10/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5554 - sparse_categorical_accuracy: 0.7935 - val_loss: 0.4222 - val_sparse_categorical_accuracy: 0.8432\n",
            "Epoch 11/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5570 - sparse_categorical_accuracy: 0.7957 - val_loss: 0.4133 - val_sparse_categorical_accuracy: 0.8443\n",
            "Epoch 12/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5508 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.3957 - val_sparse_categorical_accuracy: 0.8551\n",
            "Epoch 13/100\n",
            "71/71 [==============================] - 3s 47ms/step - loss: 0.5479 - sparse_categorical_accuracy: 0.8013 - val_loss: 0.4066 - val_sparse_categorical_accuracy: 0.8449\n",
            "Epoch 14/100\n",
            "71/71 [==============================] - 3s 49ms/step - loss: 0.5841 - sparse_categorical_accuracy: 0.7872 - val_loss: 0.4308 - val_sparse_categorical_accuracy: 0.8494\n",
            "Epoch 15/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5421 - sparse_categorical_accuracy: 0.7989 - val_loss: 0.4020 - val_sparse_categorical_accuracy: 0.8500\n",
            "Epoch 16/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5595 - sparse_categorical_accuracy: 0.7940 - val_loss: 0.3990 - val_sparse_categorical_accuracy: 0.8489\n",
            "Epoch 17/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5597 - sparse_categorical_accuracy: 0.7960 - val_loss: 0.4146 - val_sparse_categorical_accuracy: 0.8489\n",
            "Epoch 18/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5379 - sparse_categorical_accuracy: 0.8001 - val_loss: 0.4274 - val_sparse_categorical_accuracy: 0.8369\n",
            "Epoch 19/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5428 - sparse_categorical_accuracy: 0.8021 - val_loss: 0.3963 - val_sparse_categorical_accuracy: 0.8466\n",
            "Epoch 20/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5239 - sparse_categorical_accuracy: 0.7993 - val_loss: 0.4108 - val_sparse_categorical_accuracy: 0.8460\n",
            "Epoch 21/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5348 - sparse_categorical_accuracy: 0.8006 - val_loss: 0.4095 - val_sparse_categorical_accuracy: 0.8398\n",
            "Epoch 22/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5227 - sparse_categorical_accuracy: 0.8058 - val_loss: 0.4251 - val_sparse_categorical_accuracy: 0.8364\n",
            "Epoch 23/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5249 - sparse_categorical_accuracy: 0.8061 - val_loss: 0.4264 - val_sparse_categorical_accuracy: 0.8364\n",
            "Epoch 24/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5297 - sparse_categorical_accuracy: 0.8037 - val_loss: 0.3944 - val_sparse_categorical_accuracy: 0.8557\n",
            "Epoch 25/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5287 - sparse_categorical_accuracy: 0.8045 - val_loss: 0.4099 - val_sparse_categorical_accuracy: 0.8415\n",
            "Epoch 26/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5146 - sparse_categorical_accuracy: 0.8078 - val_loss: 0.4080 - val_sparse_categorical_accuracy: 0.8511\n",
            "Epoch 27/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5062 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.4064 - val_sparse_categorical_accuracy: 0.8511\n",
            "Epoch 28/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5289 - sparse_categorical_accuracy: 0.8051 - val_loss: 0.4063 - val_sparse_categorical_accuracy: 0.8494\n",
            "Epoch 29/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5209 - sparse_categorical_accuracy: 0.8050 - val_loss: 0.4093 - val_sparse_categorical_accuracy: 0.8415\n",
            "Epoch 30/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5119 - sparse_categorical_accuracy: 0.8125 - val_loss: 0.4172 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 31/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5130 - sparse_categorical_accuracy: 0.8097 - val_loss: 0.4273 - val_sparse_categorical_accuracy: 0.8301\n",
            "Epoch 32/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5023 - sparse_categorical_accuracy: 0.8121 - val_loss: 0.4202 - val_sparse_categorical_accuracy: 0.8443\n",
            "Epoch 33/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5293 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.4079 - val_sparse_categorical_accuracy: 0.8477\n",
            "Epoch 34/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5194 - sparse_categorical_accuracy: 0.8070 - val_loss: 0.4011 - val_sparse_categorical_accuracy: 0.8472\n",
            "Epoch 35/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5329 - sparse_categorical_accuracy: 0.7970 - val_loss: 0.4013 - val_sparse_categorical_accuracy: 0.8455\n",
            "Epoch 36/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5023 - sparse_categorical_accuracy: 0.8132 - val_loss: 0.4008 - val_sparse_categorical_accuracy: 0.8455\n",
            "Epoch 37/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5068 - sparse_categorical_accuracy: 0.8118 - val_loss: 0.4117 - val_sparse_categorical_accuracy: 0.8364\n",
            "Epoch 38/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5084 - sparse_categorical_accuracy: 0.8082 - val_loss: 0.4041 - val_sparse_categorical_accuracy: 0.8443\n",
            "Epoch 39/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5313 - sparse_categorical_accuracy: 0.8028 - val_loss: 0.3973 - val_sparse_categorical_accuracy: 0.8500\n",
            "Epoch 40/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5047 - sparse_categorical_accuracy: 0.8104 - val_loss: 0.3938 - val_sparse_categorical_accuracy: 0.8477\n",
            "Epoch 41/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5174 - sparse_categorical_accuracy: 0.8036 - val_loss: 0.4019 - val_sparse_categorical_accuracy: 0.8489\n",
            "Epoch 42/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5144 - sparse_categorical_accuracy: 0.8060 - val_loss: 0.3996 - val_sparse_categorical_accuracy: 0.8466\n",
            "Epoch 43/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5039 - sparse_categorical_accuracy: 0.8091 - val_loss: 0.3991 - val_sparse_categorical_accuracy: 0.8449\n",
            "Epoch 44/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5220 - sparse_categorical_accuracy: 0.8072 - val_loss: 0.4271 - val_sparse_categorical_accuracy: 0.8318\n",
            "Epoch 45/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5086 - sparse_categorical_accuracy: 0.8048 - val_loss: 0.3967 - val_sparse_categorical_accuracy: 0.8386\n",
            "Epoch 46/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4964 - sparse_categorical_accuracy: 0.8089 - val_loss: 0.3949 - val_sparse_categorical_accuracy: 0.8551\n",
            "Epoch 47/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4953 - sparse_categorical_accuracy: 0.8153 - val_loss: 0.4179 - val_sparse_categorical_accuracy: 0.8449\n",
            "Epoch 48/100\n",
            "71/71 [==============================] - 3s 49ms/step - loss: 0.5128 - sparse_categorical_accuracy: 0.8092 - val_loss: 0.3925 - val_sparse_categorical_accuracy: 0.8506\n",
            "Epoch 49/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4954 - sparse_categorical_accuracy: 0.8121 - val_loss: 0.3868 - val_sparse_categorical_accuracy: 0.8534\n",
            "Epoch 50/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4888 - sparse_categorical_accuracy: 0.8126 - val_loss: 0.3952 - val_sparse_categorical_accuracy: 0.8523\n",
            "Epoch 51/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5017 - sparse_categorical_accuracy: 0.8115 - val_loss: 0.4022 - val_sparse_categorical_accuracy: 0.8432\n",
            "Epoch 52/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4940 - sparse_categorical_accuracy: 0.8112 - val_loss: 0.4013 - val_sparse_categorical_accuracy: 0.8426\n",
            "Epoch 53/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5201 - sparse_categorical_accuracy: 0.8033 - val_loss: 0.4110 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 54/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5056 - sparse_categorical_accuracy: 0.8112 - val_loss: 0.4084 - val_sparse_categorical_accuracy: 0.8460\n",
            "Epoch 55/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5057 - sparse_categorical_accuracy: 0.8071 - val_loss: 0.3956 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 56/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4933 - sparse_categorical_accuracy: 0.8175 - val_loss: 0.4109 - val_sparse_categorical_accuracy: 0.8392\n",
            "Epoch 57/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4811 - sparse_categorical_accuracy: 0.8196 - val_loss: 0.3983 - val_sparse_categorical_accuracy: 0.8511\n",
            "Epoch 58/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5032 - sparse_categorical_accuracy: 0.8108 - val_loss: 0.4220 - val_sparse_categorical_accuracy: 0.8364\n",
            "Epoch 59/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5123 - sparse_categorical_accuracy: 0.8088 - val_loss: 0.3997 - val_sparse_categorical_accuracy: 0.8511\n",
            "Epoch 60/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4863 - sparse_categorical_accuracy: 0.8135 - val_loss: 0.3925 - val_sparse_categorical_accuracy: 0.8534\n",
            "Epoch 61/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5099 - sparse_categorical_accuracy: 0.8053 - val_loss: 0.3908 - val_sparse_categorical_accuracy: 0.8506\n",
            "Epoch 62/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4983 - sparse_categorical_accuracy: 0.8080 - val_loss: 0.3994 - val_sparse_categorical_accuracy: 0.8545\n",
            "Epoch 63/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4934 - sparse_categorical_accuracy: 0.8129 - val_loss: 0.3940 - val_sparse_categorical_accuracy: 0.8534\n",
            "Epoch 64/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.4896 - sparse_categorical_accuracy: 0.8124 - val_loss: 0.3902 - val_sparse_categorical_accuracy: 0.8489\n",
            "Epoch 65/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5087 - sparse_categorical_accuracy: 0.8071 - val_loss: 0.4122 - val_sparse_categorical_accuracy: 0.8358\n",
            "Epoch 66/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4936 - sparse_categorical_accuracy: 0.8089 - val_loss: 0.3862 - val_sparse_categorical_accuracy: 0.8477\n",
            "Epoch 67/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5145 - sparse_categorical_accuracy: 0.8112 - val_loss: 0.4049 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 68/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4963 - sparse_categorical_accuracy: 0.8112 - val_loss: 0.3985 - val_sparse_categorical_accuracy: 0.8443\n",
            "Epoch 69/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5105 - sparse_categorical_accuracy: 0.8099 - val_loss: 0.4053 - val_sparse_categorical_accuracy: 0.8375\n",
            "Epoch 70/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4980 - sparse_categorical_accuracy: 0.8098 - val_loss: 0.4002 - val_sparse_categorical_accuracy: 0.8472\n",
            "Epoch 71/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4924 - sparse_categorical_accuracy: 0.8179 - val_loss: 0.4012 - val_sparse_categorical_accuracy: 0.8472\n",
            "Epoch 72/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4924 - sparse_categorical_accuracy: 0.8139 - val_loss: 0.3864 - val_sparse_categorical_accuracy: 0.8545\n",
            "Epoch 73/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5014 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.3906 - val_sparse_categorical_accuracy: 0.8574\n",
            "Epoch 74/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5073 - sparse_categorical_accuracy: 0.8115 - val_loss: 0.3911 - val_sparse_categorical_accuracy: 0.8534\n",
            "Epoch 75/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5105 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.3972 - val_sparse_categorical_accuracy: 0.8511\n",
            "Epoch 76/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4773 - sparse_categorical_accuracy: 0.8193 - val_loss: 0.3968 - val_sparse_categorical_accuracy: 0.8455\n",
            "Epoch 77/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4849 - sparse_categorical_accuracy: 0.8142 - val_loss: 0.4009 - val_sparse_categorical_accuracy: 0.8443\n",
            "Epoch 78/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4826 - sparse_categorical_accuracy: 0.8178 - val_loss: 0.3875 - val_sparse_categorical_accuracy: 0.8523\n",
            "Epoch 79/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4907 - sparse_categorical_accuracy: 0.8151 - val_loss: 0.4016 - val_sparse_categorical_accuracy: 0.8511\n",
            "Epoch 80/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4841 - sparse_categorical_accuracy: 0.8183 - val_loss: 0.3922 - val_sparse_categorical_accuracy: 0.8511\n",
            "Epoch 81/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4834 - sparse_categorical_accuracy: 0.8129 - val_loss: 0.4025 - val_sparse_categorical_accuracy: 0.8580\n",
            "Epoch 82/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4717 - sparse_categorical_accuracy: 0.8196 - val_loss: 0.4198 - val_sparse_categorical_accuracy: 0.8426\n",
            "Epoch 83/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4898 - sparse_categorical_accuracy: 0.8139 - val_loss: 0.4014 - val_sparse_categorical_accuracy: 0.8483\n",
            "Epoch 84/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5055 - sparse_categorical_accuracy: 0.8108 - val_loss: 0.4091 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 85/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4971 - sparse_categorical_accuracy: 0.8119 - val_loss: 0.4101 - val_sparse_categorical_accuracy: 0.8426\n",
            "Epoch 86/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4930 - sparse_categorical_accuracy: 0.8097 - val_loss: 0.4087 - val_sparse_categorical_accuracy: 0.8403\n",
            "Epoch 87/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4826 - sparse_categorical_accuracy: 0.8188 - val_loss: 0.4008 - val_sparse_categorical_accuracy: 0.8432\n",
            "Epoch 88/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4997 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.3891 - val_sparse_categorical_accuracy: 0.8551\n",
            "Epoch 89/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4917 - sparse_categorical_accuracy: 0.8077 - val_loss: 0.3986 - val_sparse_categorical_accuracy: 0.8460\n",
            "Epoch 90/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4785 - sparse_categorical_accuracy: 0.8185 - val_loss: 0.3969 - val_sparse_categorical_accuracy: 0.8506\n",
            "Epoch 91/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4684 - sparse_categorical_accuracy: 0.8152 - val_loss: 0.3967 - val_sparse_categorical_accuracy: 0.8466\n",
            "Epoch 92/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4904 - sparse_categorical_accuracy: 0.8139 - val_loss: 0.3980 - val_sparse_categorical_accuracy: 0.8568\n",
            "Epoch 93/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4821 - sparse_categorical_accuracy: 0.8227 - val_loss: 0.4209 - val_sparse_categorical_accuracy: 0.8347\n",
            "Epoch 94/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4873 - sparse_categorical_accuracy: 0.8158 - val_loss: 0.3994 - val_sparse_categorical_accuracy: 0.8545\n",
            "Epoch 95/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4901 - sparse_categorical_accuracy: 0.8165 - val_loss: 0.3990 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 96/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4939 - sparse_categorical_accuracy: 0.8176 - val_loss: 0.4057 - val_sparse_categorical_accuracy: 0.8409\n",
            "Epoch 97/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4909 - sparse_categorical_accuracy: 0.8128 - val_loss: 0.3937 - val_sparse_categorical_accuracy: 0.8602\n",
            "Epoch 98/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4723 - sparse_categorical_accuracy: 0.8175 - val_loss: 0.4016 - val_sparse_categorical_accuracy: 0.8466\n",
            "Epoch 99/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4799 - sparse_categorical_accuracy: 0.8186 - val_loss: 0.4036 - val_sparse_categorical_accuracy: 0.8460\n",
            "Epoch 100/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4838 - sparse_categorical_accuracy: 0.8172 - val_loss: 0.4047 - val_sparse_categorical_accuracy: 0.8455\n"
          ]
        }
      ],
      "source": [
        "dropout_rate = 0.5\n",
        "hidden_units = 512\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "epochs = 100\n",
        "\n",
        "classifier = create_classifier(encoder, dropout_rate, hidden_units, learning_rate, trainable=False)\n",
        "classifier.summary()\n",
        "with tf.device('/gpu:0'):\n",
        "    history = classifier.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "classifier.save('classifier.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
