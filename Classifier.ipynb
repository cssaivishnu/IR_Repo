{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cssaivishnu/IR_Repo/blob/main/Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the github repo"
      ],
      "metadata": {
        "id": "y0skI-nk54UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch scratch https://github.com/cssaivishnu/IR_Repo.git"
      ],
      "metadata": {
        "id": "nVLI5KUx2HL5",
        "outputId": "8f27d191-a924-488a-f4f6-a64dda86cdf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IR_Repo'...\n",
            "remote: Enumerating objects: 63727, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 63727 (delta 20), reused 0 (delta 0), pack-reused 63687\u001b[K\n",
            "Receiving objects: 100% (63727/63727), 2.34 GiB | 43.55 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "Updating files: 100% (63288/63288), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9e4eRhtB52N"
      },
      "source": [
        "Import the essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "1tV0FUOQgnJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f488f1e0-3ba2-46d7-fdce-d633b77c6d82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.0)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gkxVB3cmG3lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f60fd42-1193-469a-ddbd-a6a57754341e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "tf.random.set_seed(20)\n",
        "random.seed = 20\n",
        "np.random.seed = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqF-6BAL4387"
      },
      "source": [
        "Make the main repo as the current active repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12a38c6-6bd5-4569-993f-249862013c73",
        "id": "qqKEWeFM4387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/IR_Repo\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/IR_Repo')\n",
        "cwd = os.getcwd()\n",
        "print(cwd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3vWKcjbY9R9"
      },
      "source": [
        "From the complete dataset of approximately 63285 images from 35 categories, we consider only the product categories with atleast 150 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cOoxFhBQN1w2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad0e4cb-989f-425a-9659-187b1052bfe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of categories: 35\n",
            "Number of categories with atleast 150 images: 22\n",
            "Number of categories with 600 images: 18\n",
            "Total Number of Images: 63285\n"
          ]
        }
      ],
      "source": [
        "dir_path = 'atlas_dataset_full'\n",
        "dir_count = 0\n",
        "active_dir_count = 0\n",
        "complete_dir_count = 0\n",
        "total_images = 0\n",
        "images_list = []\n",
        "\n",
        "os.mkdir('dataset')\n",
        "\n",
        "for name in os.listdir(dir_path):\n",
        "    if os.path.isdir(os.path.join(dir_path, name)):\n",
        "        path = os.path.join(dir_path, name)\n",
        "        path = os.path.join(path, 'images')\n",
        "        num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
        "        if(num_files > 150):\n",
        "            active_dir_count = active_dir_count + 1\n",
        "            new_path = os.path.join('dataset', name)\n",
        "            os.mkdir(new_path)\n",
        "            i = 0\n",
        "            for f in os.listdir(path):\n",
        "                if i == 600:\n",
        "                    complete_dir_count = complete_dir_count + 1\n",
        "                    break\n",
        "                if os.path.isfile(os.path.join(path, f)):\n",
        "                    i = i + 1\n",
        "                    src = os.path.join(path, f)\n",
        "                    fname = '{:03d}'.format(i) + '.' + f.split('.')[-1]\n",
        "                    dst = os.path.join(new_path, fname)\n",
        "                    shutil.copy2(src, dst)\n",
        "            images_list.append(i)\n",
        "        total_images = total_images + num_files\n",
        "        dir_count += 1\n",
        "\n",
        "print(\"Total Number of categories:\", dir_count)\n",
        "print(\"Number of categories with atleast 150 images:\", active_dir_count)\n",
        "print(\"Number of categories with 600 images:\", complete_dir_count)\n",
        "print(\"Total Number of Images:\", total_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We find that 22 out of these 35 categories only have atleast 150 images\n",
        "\n",
        "We want to have 600 images from each of the 22 product categories, out of which 4 of them have less than 600 images.\n",
        "\n",
        "Now, we will perform image augmentation to increase the number of images in those 4 product categories with less than 600 images to 600 images"
      ],
      "metadata": {
        "id": "p9-2Y9LG_df2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a2b_YKONJFek"
      },
      "outputs": [],
      "source": [
        "# Here, the image is flipped horizontally to create a new image\n",
        "\n",
        "def horizontalflipping_augmentation(path, new_path):\n",
        "    # Define the horizontal flipping transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=1),\n",
        "    ])\n",
        "\n",
        "    # Load the image\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # Apply the horizontal flipping transformation\n",
        "    img_flipped = transform(img)\n",
        "\n",
        "    # Display the original and flipped images\n",
        "    # img.show()\n",
        "    # img_flipped.show()\n",
        "\n",
        "    # Save the horizontally flipped image\n",
        "    img.save(new_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bISEXc2PFUaY"
      },
      "outputs": [],
      "source": [
        "# Here, the image is modified by varying color glittering entities like contrast, brightness etc.\n",
        "\n",
        "def colorgittering_augmentation(path,new_path):\n",
        "    # Load the image\n",
        "    img = cv2.imread(path)\n",
        "\n",
        "    # Define the range of color jittering values\n",
        "    brightness = 0.1\n",
        "    contrast = 0.1\n",
        "    saturation = 0.1\n",
        "    hue = 0.1\n",
        "\n",
        "    # Convert the image from BGR to HSV color space\n",
        "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Apply color jittering to the image\n",
        "    img_hsv[:, :, 2] = np.clip(img_hsv[:, :, 2] * (1 + brightness), 0, 255)\n",
        "    img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1] * (1 + contrast), 0, 255)\n",
        "    img_hsv[:, :, 1] = np.clip(img_hsv[:, :, 1] * (1 + saturation), 0, 255)\n",
        "    img_hsv[:, :, 0] = np.clip(img_hsv[:, :, 0] * (1 + hue), 0, 255)\n",
        "\n",
        "    # Convert the image back to BGR color space\n",
        "    img_jittered = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    # Display the original and jittered images\n",
        "    # cv2_imshow(img)\n",
        "    # cv2_imshow(img_jittered)\n",
        "    # cv2.waitKey(0)\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "    # Save the color gritted image\n",
        "    cv2.imwrite(new_path, img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XcdGedzTOWDM"
      },
      "outputs": [],
      "source": [
        "# Here, the image is randomly scaled to a feasible size\n",
        "\n",
        "def randomscaling_augmentation(path,new_path):\n",
        "    # Load the image\n",
        "    img = Image.open(path)\n",
        "    width, height = img.size\n",
        "    size = int(min(width,height)*0.9)\n",
        "\n",
        "    # Define the random scaling transformation\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=size, scale=(0.8, 1.0)),\n",
        "    ])\n",
        "\n",
        "    # Apply the random scaling transformation\n",
        "    img_scaled = transform(img)\n",
        "\n",
        "    # Display the original and scaled images\n",
        "    # img.show()\n",
        "    # img_scaled.show()\n",
        "\n",
        "    # Save the randomly scaled image\n",
        "    img.save(new_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8EZYZlEw-zm3"
      },
      "outputs": [],
      "source": [
        "dir_path = 'dataset'\n",
        "images_list = []\n",
        "\n",
        "for name in os.listdir(dir_path):\n",
        "    if os.path.isdir(os.path.join(dir_path, name)):\n",
        "        path = os.path.join(dir_path, name)\n",
        "        num_files = len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))])\n",
        "        images_list.append(num_files)\n",
        "        if num_files != 600:\n",
        "            # print(num_files)\n",
        "            # print(name)\n",
        "            for f in os.listdir(path):\n",
        "                if os.path.isfile(os.path.join(path, f)):\n",
        "                    img_name = int(f.split('.')[0])\n",
        "                    for i in range(1,4):\n",
        "                        new_img_name = img_name + num_files*i\n",
        "                        if new_img_name > 600:\n",
        "                            break\n",
        "                        new_img_name = '{:03d}'.format(new_img_name) + '.' + f.split('.')[-1]\n",
        "                        img_path = os.path.join(path,f)\n",
        "                        new_img_path = os.path.join(path,new_img_name)\n",
        "                        if i == 1:\n",
        "                            horizontalflipping_augmentation(img_path,new_img_path)\n",
        "                        if i == 2:\n",
        "                            colorgittering_augmentation(img_path,new_img_path)\n",
        "                        if i == 3:\n",
        "                            randomscaling_augmentation(img_path,new_img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dBb7kdLvGf1"
      },
      "source": [
        "Create the label encodings mapping the product categories and save them as JSON files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = 'dataset'\n",
        "i = 0\n",
        "name_to_label = dict()\n",
        "label_to_name = dict()\n",
        "\n",
        "for name in os.listdir(dir_path):\n",
        "    if os.path.isdir(os.path.join(dir_path, name)):\n",
        "        name_to_label[name] = i\n",
        "        label_to_name[i] = name\n",
        "        i = i + 1\n",
        "\n",
        "with open('name_to_label.json', 'w') as f:\n",
        "    json.dump(name_to_label, f)\n",
        "with open('label_to_name.json', 'w') as f:\n",
        "    json.dump(label_to_name, f)"
      ],
      "metadata": {
        "id": "eDllHTjgUGMJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA_aLZBZwCRs"
      },
      "source": [
        "Load 500 out of 600 images from each product category and modify them to (56, 56) shape for training and evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MdWasMiGBIoY"
      },
      "outputs": [],
      "source": [
        "dir_path = 'dataset'\n",
        "dataset = []\n",
        "names_list = os.listdir(dir_path)\n",
        "image_shape = (56, 56)\n",
        "\n",
        "for name in names_list:\n",
        "    if os.path.isdir(os.path.join(dir_path, name)):\n",
        "        path = os.path.join(dir_path, name)\n",
        "        # print(name_to_label[name])\n",
        "        for f in os.listdir(path):\n",
        "            if os.path.isfile(os.path.join(path, f)):\n",
        "                num = int(f.split('.')[0])\n",
        "                if num > 500:\n",
        "                    continue\n",
        "                img_path = os.path.join(path, f)\n",
        "                img = Image.open(img_path)\n",
        "                img = img.resize(image_shape, Image.ANTIALIAS)\n",
        "                pixels = img.load()\n",
        "                lst = []\n",
        "                for i in range(img.size[0]):\n",
        "                    lst1 = []\n",
        "                    for j in range(img.size[1]):\n",
        "                        lst1.append(list(pixels[i, j]))\n",
        "                    lst.append(lst1)\n",
        "                lst = np.array(lst)\n",
        "                tupl = (lst,name_to_label[name])\n",
        "                dataset.append(tupl)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle the dataset for randomness and split the dataset into features(X) and labels(y)"
      ],
      "metadata": {
        "id": "RHqNOE3pIMTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(dataset)\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for tupl in dataset:\n",
        "    x, yy = tupl\n",
        "    X.append(x)\n",
        "    y.append(yy)\n",
        "X = np.array(X)\n",
        "y = np.array(y).reshape(11000,1)\n",
        "\n",
        "# Just clear the space\n",
        "dataset = []"
      ],
      "metadata": {
        "id": "ebBGsaQz4YBr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the features(X) and labels(y) into numpy files so that we do not need to load the images again and again"
      ],
      "metadata": {
        "id": "1sfYATOU5CL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('features.npy', X)\n",
        "np.save('labels.npy', y)"
      ],
      "metadata": {
        "id": "8uE13yt_4iyM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ank6_wq74388"
      },
      "source": [
        "Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (56, 56, 3)\n",
        "names_list = os.listdir('dataset')\n",
        "num_categories = len(names_list)"
      ],
      "metadata": {
        "id": "C51FYTJiNkgt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the name_to_label and label_to_name encodings"
      ],
      "metadata": {
        "id": "4nZkjl0fPCL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('name_to_label.json', 'r') as f:\n",
        "    name_to_label = json.load(f)\n",
        "with open('label_to_name.json', 'r') as f:\n",
        "    label_to_name = json.load(f)"
      ],
      "metadata": {
        "id": "N0S37j2xPDNP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the features(X) and labels(y) from their respective numpy files"
      ],
      "metadata": {
        "id": "NRS1mudkPFuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.load('features.npy')\n",
        "y = np.load('labels.npy')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "print('X_train:', X_train.shape)\n",
        "print('X_test:', X_test.shape)\n",
        "print('y_train:', y_train.shape)\n",
        "print('y_test:', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyQrQMexPKFy",
        "outputId": "3b688eea-1d60-4a75-daab-f9967fae393d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (8800, 56, 56, 3)\n",
            "X_test: (2200, 56, 56, 3)\n",
            "y_train: (8800, 1)\n",
            "y_test: (2200, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Data Augmentation"
      ],
      "metadata": {
        "id": "mJhF1icQPgqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip('horizontal'),\n",
        "        layers.RandomRotation(0.02),\n",
        "        layers.RandomWidth(0.2),\n",
        "        layers.RandomHeight(0.2)\n",
        "    ]\n",
        ")\n",
        "\n",
        "data_augmentation.layers[0].adapt(X_train)"
      ],
      "metadata": {
        "id": "UMt-JY3iLtq2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Supervised Contrastive Loss Function that will be used in training the model "
      ],
      "metadata": {
        "id": "xsxbj2dvRFXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # normalize the feature vectors\n",
        "        feature_vectors_normailzed = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute Logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normailzed, tf.transpose(feature_vectors_normailzed)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
      ],
      "metadata": {
        "id": "rR3Y2t3wS4pQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an encoder to encode the images pixel data using the RNN - ResNet50V2"
      ],
      "metadata": {
        "id": "j0ES_vSeYNZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_encoder():\n",
        "    resnet = keras.applications.ResNet50V2(\n",
        "        include_top=False, weights=None, input_shape=input_shape, pooling='avg'\n",
        "    )\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    outputs = resnet(augmented)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='encoder')\n",
        "    return model"
      ],
      "metadata": {
        "id": "vfqUfbAOXeOK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a Projection head"
      ],
      "metadata": {
        "id": "XSpCGwfQdAEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_projection_head(encoder, projection_units):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    outputs = layers.Dense(projection_units, activation='relu')(features)\n",
        "    model = keras.Model(\n",
        "        inputs=inputs, outputs=outputs, name='encoder_with_projection_head'\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "QkKIGzMEaehd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the encoder with Supervised Contrastive Loss defined above for better encoding"
      ],
      "metadata": {
        "id": "Hrr30sSHfMig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "projection_units = 256\n",
        "epochs = 50\n",
        "temperature = 0.05\n",
        "\n",
        "encoder = create_encoder()\n",
        "encoder.summary()\n",
        "encoder_with_projection_head = add_projection_head(encoder, projection_units)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature)\n",
        ")\n",
        "encoder_with_projection_head.summary()\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    history = encoder_with_projection_head.fit(\n",
        "        x=X_train, y=y_train, batch_size=batch_size, epochs=epochs\n",
        "    )"
      ],
      "metadata": {
        "id": "lEwNpfm5d2nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9741076e-06d2-4386-bc9f-adaa7cbea843"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 56, 56, 3)]       0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, None, None, 3)     7         \n",
            "                                                                 \n",
            " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,564,807\n",
            "Trainable params: 23,519,360\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Model: \"encoder_with_projection_head\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 56, 56, 3)]       0         \n",
            "                                                                 \n",
            " encoder (Functional)        (None, 2048)              23564807  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               524544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,089,351\n",
            "Trainable params: 24,043,904\n",
            "Non-trainable params: 45,447\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "88/88 [==============================] - 85s 453ms/step - loss: 4.1599\n",
            "Epoch 2/50\n",
            "88/88 [==============================] - 14s 158ms/step - loss: 3.7312\n",
            "Epoch 3/50\n",
            "88/88 [==============================] - 12s 140ms/step - loss: 3.4912\n",
            "Epoch 4/50\n",
            "88/88 [==============================] - 13s 144ms/step - loss: 3.3784\n",
            "Epoch 5/50\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 3.2769\n",
            "Epoch 6/50\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 3.1830\n",
            "Epoch 7/50\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 3.1301\n",
            "Epoch 8/50\n",
            "88/88 [==============================] - 11s 121ms/step - loss: 3.0793\n",
            "Epoch 9/50\n",
            "88/88 [==============================] - 11s 131ms/step - loss: 3.0073\n",
            "Epoch 10/50\n",
            "88/88 [==============================] - 10s 114ms/step - loss: 2.9913\n",
            "Epoch 11/50\n",
            "88/88 [==============================] - 10s 112ms/step - loss: 2.9244\n",
            "Epoch 12/50\n",
            "88/88 [==============================] - 10s 115ms/step - loss: 2.8876\n",
            "Epoch 13/50\n",
            "88/88 [==============================] - 10s 113ms/step - loss: 2.8584\n",
            "Epoch 14/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.8349\n",
            "Epoch 15/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.7814\n",
            "Epoch 16/50\n",
            "88/88 [==============================] - 10s 115ms/step - loss: 2.7486\n",
            "Epoch 17/50\n",
            "88/88 [==============================] - 9s 108ms/step - loss: 2.7507\n",
            "Epoch 18/50\n",
            "88/88 [==============================] - 9s 108ms/step - loss: 2.7247\n",
            "Epoch 19/50\n",
            "88/88 [==============================] - 10s 110ms/step - loss: 2.6837\n",
            "Epoch 20/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.6533\n",
            "Epoch 21/50\n",
            "88/88 [==============================] - 9s 103ms/step - loss: 2.6687\n",
            "Epoch 22/50\n",
            "88/88 [==============================] - 10s 108ms/step - loss: 2.6431\n",
            "Epoch 23/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.5929\n",
            "Epoch 24/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.5831\n",
            "Epoch 25/50\n",
            "88/88 [==============================] - 10s 111ms/step - loss: 2.5620\n",
            "Epoch 26/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.5318\n",
            "Epoch 27/50\n",
            "88/88 [==============================] - 9s 103ms/step - loss: 2.5436\n",
            "Epoch 28/50\n",
            "88/88 [==============================] - 9s 104ms/step - loss: 2.5149\n",
            "Epoch 29/50\n",
            "88/88 [==============================] - 10s 109ms/step - loss: 2.5184\n",
            "Epoch 30/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.4835\n",
            "Epoch 31/50\n",
            "88/88 [==============================] - 10s 112ms/step - loss: 2.4830\n",
            "Epoch 32/50\n",
            "88/88 [==============================] - 9s 104ms/step - loss: 2.4540\n",
            "Epoch 33/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.4681\n",
            "Epoch 34/50\n",
            "88/88 [==============================] - 9s 104ms/step - loss: 2.4359\n",
            "Epoch 35/50\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 2.4234\n",
            "Epoch 36/50\n",
            "88/88 [==============================] - 9s 104ms/step - loss: 2.4235\n",
            "Epoch 37/50\n",
            "88/88 [==============================] - 9s 102ms/step - loss: 2.4096\n",
            "Epoch 38/50\n",
            "88/88 [==============================] - 10s 111ms/step - loss: 2.3948\n",
            "Epoch 39/50\n",
            "88/88 [==============================] - 9s 103ms/step - loss: 2.3764\n",
            "Epoch 40/50\n",
            "88/88 [==============================] - 9s 104ms/step - loss: 2.3840\n",
            "Epoch 41/50\n",
            "88/88 [==============================] - 9s 103ms/step - loss: 2.3418\n",
            "Epoch 42/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.3384\n",
            "Epoch 43/50\n",
            "88/88 [==============================] - 9s 106ms/step - loss: 2.3543\n",
            "Epoch 44/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.3564\n",
            "Epoch 45/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.3147\n",
            "Epoch 46/50\n",
            "88/88 [==============================] - 9s 102ms/step - loss: 2.3101\n",
            "Epoch 47/50\n",
            "88/88 [==============================] - 9s 103ms/step - loss: 2.2936\n",
            "Epoch 48/50\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.2907\n",
            "Epoch 49/50\n",
            "88/88 [==============================] - 9s 103ms/step - loss: 2.2975\n",
            "Epoch 50/50\n",
            "88/88 [==============================] - 9s 104ms/step - loss: 2.2739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating and training the classifier using the pretrained encoder"
      ],
      "metadata": {
        "id": "hEU6SyBaBwj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_classifier(encoder, dropout_rate, hidden_units, learning_rate, trainable=True):\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = trainable\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    features = layers.Dense(hidden_units, activation='relu')(features)\n",
        "    features = layers.Dropout(dropout_rate)(features)\n",
        "    outputs = layers.Dense(num_categories, activation='softmax')(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name = 'classifier')\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "FERzd63ofL12"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.5\n",
        "hidden_units = 512\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "epochs = 100\n",
        "\n",
        "classifier = create_classifier(encoder, dropout_rate, hidden_units, learning_rate, trainable=False)\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "    history = classifier.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "accuracy = classifier.evaluate(X_test, y_test)[1]\n",
        "classifier.save('classifier.h5')"
      ],
      "metadata": {
        "id": "C_Mq2HYPN5uo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b483abc6-c187-4b7b-f0c9-ace167b11e38"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "71/71 [==============================] - 10s 74ms/step - loss: 0.9716 - sparse_categorical_accuracy: 0.7067 - val_loss: 0.5263 - val_sparse_categorical_accuracy: 0.8068\n",
            "Epoch 2/100\n",
            "71/71 [==============================] - 3s 48ms/step - loss: 0.7001 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5328 - val_sparse_categorical_accuracy: 0.8062\n",
            "Epoch 3/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.6472 - sparse_categorical_accuracy: 0.7744 - val_loss: 0.4899 - val_sparse_categorical_accuracy: 0.8199\n",
            "Epoch 4/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.6164 - sparse_categorical_accuracy: 0.7790 - val_loss: 0.4656 - val_sparse_categorical_accuracy: 0.8295\n",
            "Epoch 5/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5976 - sparse_categorical_accuracy: 0.7825 - val_loss: 0.4499 - val_sparse_categorical_accuracy: 0.8295\n",
            "Epoch 6/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5790 - sparse_categorical_accuracy: 0.7915 - val_loss: 0.4691 - val_sparse_categorical_accuracy: 0.8239\n",
            "Epoch 7/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5912 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4660 - val_sparse_categorical_accuracy: 0.8295\n",
            "Epoch 8/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5761 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4601 - val_sparse_categorical_accuracy: 0.8318\n",
            "Epoch 9/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5460 - sparse_categorical_accuracy: 0.7962 - val_loss: 0.4620 - val_sparse_categorical_accuracy: 0.8233\n",
            "Epoch 10/100\n",
            "71/71 [==============================] - 4s 53ms/step - loss: 0.5631 - sparse_categorical_accuracy: 0.7949 - val_loss: 0.4651 - val_sparse_categorical_accuracy: 0.8261\n",
            "Epoch 11/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5421 - sparse_categorical_accuracy: 0.8027 - val_loss: 0.4551 - val_sparse_categorical_accuracy: 0.8284\n",
            "Epoch 12/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5738 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4622 - val_sparse_categorical_accuracy: 0.8216\n",
            "Epoch 13/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5577 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4482 - val_sparse_categorical_accuracy: 0.8261\n",
            "Epoch 14/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5503 - sparse_categorical_accuracy: 0.7929 - val_loss: 0.4489 - val_sparse_categorical_accuracy: 0.8284\n",
            "Epoch 15/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5571 - sparse_categorical_accuracy: 0.7960 - val_loss: 0.4507 - val_sparse_categorical_accuracy: 0.8278\n",
            "Epoch 16/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5385 - sparse_categorical_accuracy: 0.7967 - val_loss: 0.4374 - val_sparse_categorical_accuracy: 0.8398\n",
            "Epoch 17/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5211 - sparse_categorical_accuracy: 0.8050 - val_loss: 0.4530 - val_sparse_categorical_accuracy: 0.8273\n",
            "Epoch 18/100\n",
            "71/71 [==============================] - 4s 50ms/step - loss: 0.5320 - sparse_categorical_accuracy: 0.8013 - val_loss: 0.4378 - val_sparse_categorical_accuracy: 0.8295\n",
            "Epoch 19/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5131 - sparse_categorical_accuracy: 0.8099 - val_loss: 0.4297 - val_sparse_categorical_accuracy: 0.8318\n",
            "Epoch 20/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5212 - sparse_categorical_accuracy: 0.8053 - val_loss: 0.4500 - val_sparse_categorical_accuracy: 0.8313\n",
            "Epoch 21/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5527 - sparse_categorical_accuracy: 0.7911 - val_loss: 0.4445 - val_sparse_categorical_accuracy: 0.8261\n",
            "Epoch 22/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5283 - sparse_categorical_accuracy: 0.8011 - val_loss: 0.4507 - val_sparse_categorical_accuracy: 0.8352\n",
            "Epoch 23/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5281 - sparse_categorical_accuracy: 0.8084 - val_loss: 0.4458 - val_sparse_categorical_accuracy: 0.8330\n",
            "Epoch 24/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5298 - sparse_categorical_accuracy: 0.8006 - val_loss: 0.4493 - val_sparse_categorical_accuracy: 0.8159\n",
            "Epoch 25/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5148 - sparse_categorical_accuracy: 0.8040 - val_loss: 0.4458 - val_sparse_categorical_accuracy: 0.8182\n",
            "Epoch 26/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5086 - sparse_categorical_accuracy: 0.8060 - val_loss: 0.4352 - val_sparse_categorical_accuracy: 0.8290\n",
            "Epoch 27/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5358 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.4506 - val_sparse_categorical_accuracy: 0.8284\n",
            "Epoch 28/100\n",
            "71/71 [==============================] - 3s 47ms/step - loss: 0.5400 - sparse_categorical_accuracy: 0.7960 - val_loss: 0.4576 - val_sparse_categorical_accuracy: 0.8273\n",
            "Epoch 29/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5255 - sparse_categorical_accuracy: 0.7946 - val_loss: 0.4487 - val_sparse_categorical_accuracy: 0.8284\n",
            "Epoch 30/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5126 - sparse_categorical_accuracy: 0.8065 - val_loss: 0.4559 - val_sparse_categorical_accuracy: 0.8267\n",
            "Epoch 31/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5243 - sparse_categorical_accuracy: 0.8068 - val_loss: 0.4790 - val_sparse_categorical_accuracy: 0.8199\n",
            "Epoch 32/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5321 - sparse_categorical_accuracy: 0.8062 - val_loss: 0.4379 - val_sparse_categorical_accuracy: 0.8278\n",
            "Epoch 33/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5101 - sparse_categorical_accuracy: 0.8081 - val_loss: 0.4401 - val_sparse_categorical_accuracy: 0.8318\n",
            "Epoch 34/100\n",
            "71/71 [==============================] - 3s 48ms/step - loss: 0.5436 - sparse_categorical_accuracy: 0.7984 - val_loss: 0.4319 - val_sparse_categorical_accuracy: 0.8352\n",
            "Epoch 35/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5179 - sparse_categorical_accuracy: 0.8077 - val_loss: 0.4263 - val_sparse_categorical_accuracy: 0.8381\n",
            "Epoch 36/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5031 - sparse_categorical_accuracy: 0.8149 - val_loss: 0.4526 - val_sparse_categorical_accuracy: 0.8347\n",
            "Epoch 37/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.5119 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.4278 - val_sparse_categorical_accuracy: 0.8335\n",
            "Epoch 38/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5208 - sparse_categorical_accuracy: 0.7982 - val_loss: 0.4469 - val_sparse_categorical_accuracy: 0.8284\n",
            "Epoch 39/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5129 - sparse_categorical_accuracy: 0.7994 - val_loss: 0.4445 - val_sparse_categorical_accuracy: 0.8295\n",
            "Epoch 40/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4990 - sparse_categorical_accuracy: 0.8101 - val_loss: 0.4456 - val_sparse_categorical_accuracy: 0.8239\n",
            "Epoch 41/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5018 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.4432 - val_sparse_categorical_accuracy: 0.8330\n",
            "Epoch 42/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5296 - sparse_categorical_accuracy: 0.8034 - val_loss: 0.4381 - val_sparse_categorical_accuracy: 0.8335\n",
            "Epoch 43/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5071 - sparse_categorical_accuracy: 0.8026 - val_loss: 0.4488 - val_sparse_categorical_accuracy: 0.8318\n",
            "Epoch 44/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5119 - sparse_categorical_accuracy: 0.8126 - val_loss: 0.4466 - val_sparse_categorical_accuracy: 0.8256\n",
            "Epoch 45/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5100 - sparse_categorical_accuracy: 0.8098 - val_loss: 0.4705 - val_sparse_categorical_accuracy: 0.8273\n",
            "Epoch 46/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5239 - sparse_categorical_accuracy: 0.8031 - val_loss: 0.4433 - val_sparse_categorical_accuracy: 0.8398\n",
            "Epoch 47/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.5053 - sparse_categorical_accuracy: 0.8104 - val_loss: 0.4404 - val_sparse_categorical_accuracy: 0.8313\n",
            "Epoch 48/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4978 - sparse_categorical_accuracy: 0.8101 - val_loss: 0.4467 - val_sparse_categorical_accuracy: 0.8313\n",
            "Epoch 49/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4945 - sparse_categorical_accuracy: 0.8122 - val_loss: 0.4411 - val_sparse_categorical_accuracy: 0.8386\n",
            "Epoch 50/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.4946 - sparse_categorical_accuracy: 0.8121 - val_loss: 0.4298 - val_sparse_categorical_accuracy: 0.8375\n",
            "Epoch 51/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4994 - sparse_categorical_accuracy: 0.8119 - val_loss: 0.4325 - val_sparse_categorical_accuracy: 0.8352\n",
            "Epoch 52/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5037 - sparse_categorical_accuracy: 0.8116 - val_loss: 0.4401 - val_sparse_categorical_accuracy: 0.8347\n",
            "Epoch 53/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4997 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.4629 - val_sparse_categorical_accuracy: 0.8244\n",
            "Epoch 54/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4976 - sparse_categorical_accuracy: 0.8114 - val_loss: 0.4370 - val_sparse_categorical_accuracy: 0.8290\n",
            "Epoch 55/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5125 - sparse_categorical_accuracy: 0.8045 - val_loss: 0.4475 - val_sparse_categorical_accuracy: 0.8273\n",
            "Epoch 56/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5088 - sparse_categorical_accuracy: 0.8068 - val_loss: 0.4339 - val_sparse_categorical_accuracy: 0.8261\n",
            "Epoch 57/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5208 - sparse_categorical_accuracy: 0.8051 - val_loss: 0.4419 - val_sparse_categorical_accuracy: 0.8341\n",
            "Epoch 58/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5015 - sparse_categorical_accuracy: 0.8102 - val_loss: 0.4213 - val_sparse_categorical_accuracy: 0.8347\n",
            "Epoch 59/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4913 - sparse_categorical_accuracy: 0.8135 - val_loss: 0.4154 - val_sparse_categorical_accuracy: 0.8403\n",
            "Epoch 60/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4959 - sparse_categorical_accuracy: 0.8135 - val_loss: 0.4410 - val_sparse_categorical_accuracy: 0.8273\n",
            "Epoch 61/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5079 - sparse_categorical_accuracy: 0.8141 - val_loss: 0.4509 - val_sparse_categorical_accuracy: 0.8227\n",
            "Epoch 62/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5033 - sparse_categorical_accuracy: 0.8057 - val_loss: 0.4383 - val_sparse_categorical_accuracy: 0.8409\n",
            "Epoch 63/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.5108 - sparse_categorical_accuracy: 0.8048 - val_loss: 0.4402 - val_sparse_categorical_accuracy: 0.8278\n",
            "Epoch 64/100\n",
            "71/71 [==============================] - 3s 46ms/step - loss: 0.4951 - sparse_categorical_accuracy: 0.8124 - val_loss: 0.4442 - val_sparse_categorical_accuracy: 0.8318\n",
            "Epoch 65/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5040 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.4458 - val_sparse_categorical_accuracy: 0.8330\n",
            "Epoch 66/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4963 - sparse_categorical_accuracy: 0.8112 - val_loss: 0.4318 - val_sparse_categorical_accuracy: 0.8364\n",
            "Epoch 67/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.5186 - sparse_categorical_accuracy: 0.8020 - val_loss: 0.4373 - val_sparse_categorical_accuracy: 0.8381\n",
            "Epoch 68/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.5018 - sparse_categorical_accuracy: 0.8064 - val_loss: 0.4702 - val_sparse_categorical_accuracy: 0.8278\n",
            "Epoch 69/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5016 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.4510 - val_sparse_categorical_accuracy: 0.8216\n",
            "Epoch 70/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4891 - sparse_categorical_accuracy: 0.8193 - val_loss: 0.4325 - val_sparse_categorical_accuracy: 0.8290\n",
            "Epoch 71/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4986 - sparse_categorical_accuracy: 0.8101 - val_loss: 0.4242 - val_sparse_categorical_accuracy: 0.8403\n",
            "Epoch 72/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4881 - sparse_categorical_accuracy: 0.8178 - val_loss: 0.4461 - val_sparse_categorical_accuracy: 0.8284\n",
            "Epoch 73/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4843 - sparse_categorical_accuracy: 0.8156 - val_loss: 0.4518 - val_sparse_categorical_accuracy: 0.8284\n",
            "Epoch 74/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4858 - sparse_categorical_accuracy: 0.8124 - val_loss: 0.4651 - val_sparse_categorical_accuracy: 0.8352\n",
            "Epoch 75/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4958 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.4300 - val_sparse_categorical_accuracy: 0.8375\n",
            "Epoch 76/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4715 - sparse_categorical_accuracy: 0.8203 - val_loss: 0.4457 - val_sparse_categorical_accuracy: 0.8341\n",
            "Epoch 77/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4908 - sparse_categorical_accuracy: 0.8138 - val_loss: 0.4278 - val_sparse_categorical_accuracy: 0.8278\n",
            "Epoch 78/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4911 - sparse_categorical_accuracy: 0.8134 - val_loss: 0.4448 - val_sparse_categorical_accuracy: 0.8273\n",
            "Epoch 79/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.4693 - sparse_categorical_accuracy: 0.8183 - val_loss: 0.4308 - val_sparse_categorical_accuracy: 0.8386\n",
            "Epoch 80/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4861 - sparse_categorical_accuracy: 0.8148 - val_loss: 0.4393 - val_sparse_categorical_accuracy: 0.8381\n",
            "Epoch 81/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4808 - sparse_categorical_accuracy: 0.8173 - val_loss: 0.4400 - val_sparse_categorical_accuracy: 0.8369\n",
            "Epoch 82/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4938 - sparse_categorical_accuracy: 0.8119 - val_loss: 0.4431 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 83/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4805 - sparse_categorical_accuracy: 0.8162 - val_loss: 0.4423 - val_sparse_categorical_accuracy: 0.8295\n",
            "Epoch 84/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4811 - sparse_categorical_accuracy: 0.8192 - val_loss: 0.4320 - val_sparse_categorical_accuracy: 0.8307\n",
            "Epoch 85/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4864 - sparse_categorical_accuracy: 0.8170 - val_loss: 0.4344 - val_sparse_categorical_accuracy: 0.8324\n",
            "Epoch 86/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.4923 - sparse_categorical_accuracy: 0.8138 - val_loss: 0.4451 - val_sparse_categorical_accuracy: 0.8341\n",
            "Epoch 87/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4927 - sparse_categorical_accuracy: 0.8118 - val_loss: 0.4271 - val_sparse_categorical_accuracy: 0.8341\n",
            "Epoch 88/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4951 - sparse_categorical_accuracy: 0.8107 - val_loss: 0.4322 - val_sparse_categorical_accuracy: 0.8398\n",
            "Epoch 89/100\n",
            "71/71 [==============================] - 3s 45ms/step - loss: 0.4947 - sparse_categorical_accuracy: 0.8141 - val_loss: 0.4393 - val_sparse_categorical_accuracy: 0.8420\n",
            "Epoch 90/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.5030 - sparse_categorical_accuracy: 0.8105 - val_loss: 0.4295 - val_sparse_categorical_accuracy: 0.8409\n",
            "Epoch 91/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4884 - sparse_categorical_accuracy: 0.8143 - val_loss: 0.4500 - val_sparse_categorical_accuracy: 0.8341\n",
            "Epoch 92/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4707 - sparse_categorical_accuracy: 0.8169 - val_loss: 0.4538 - val_sparse_categorical_accuracy: 0.8273\n",
            "Epoch 93/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.4879 - sparse_categorical_accuracy: 0.8196 - val_loss: 0.4388 - val_sparse_categorical_accuracy: 0.8330\n",
            "Epoch 94/100\n",
            "71/71 [==============================] - 3s 42ms/step - loss: 0.4721 - sparse_categorical_accuracy: 0.8196 - val_loss: 0.4545 - val_sparse_categorical_accuracy: 0.8290\n",
            "Epoch 95/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4910 - sparse_categorical_accuracy: 0.8161 - val_loss: 0.4461 - val_sparse_categorical_accuracy: 0.8261\n",
            "Epoch 96/100\n",
            "71/71 [==============================] - 3s 39ms/step - loss: 0.5115 - sparse_categorical_accuracy: 0.8037 - val_loss: 0.4713 - val_sparse_categorical_accuracy: 0.8239\n",
            "Epoch 97/100\n",
            "71/71 [==============================] - 3s 43ms/step - loss: 0.5026 - sparse_categorical_accuracy: 0.8077 - val_loss: 0.4605 - val_sparse_categorical_accuracy: 0.8313\n",
            "Epoch 98/100\n",
            "71/71 [==============================] - 3s 44ms/step - loss: 0.4744 - sparse_categorical_accuracy: 0.8175 - val_loss: 0.4475 - val_sparse_categorical_accuracy: 0.8318\n",
            "Epoch 99/100\n",
            "71/71 [==============================] - 3s 40ms/step - loss: 0.4794 - sparse_categorical_accuracy: 0.8152 - val_loss: 0.4794 - val_sparse_categorical_accuracy: 0.8295\n",
            "Epoch 100/100\n",
            "71/71 [==============================] - 3s 41ms/step - loss: 0.4906 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.4284 - val_sparse_categorical_accuracy: 0.8364\n",
            "69/69 [==============================] - 2s 22ms/step - loss: 0.9116 - sparse_categorical_accuracy: 0.7500\n",
            "Test Accuracy: 75.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Test Accuracy: {round(accuracy*100,2)}%')"
      ],
      "metadata": {
        "id": "ciH3-UQO1tUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21851090-1a77-4233-ff87-42d5d72f4782"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 75.0%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}